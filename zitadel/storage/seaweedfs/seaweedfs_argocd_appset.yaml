---
# webapp is deployed 3rd because we need secrets and persistent volumes up first
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: zitadel-seaweedfs-appset
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  goTemplate: true
  # generator allows us to source specific values from an external k8s secret
  generators:
    - plugin:
        configMapRef:
          name: secret-var-plugin-generator
        input:
          parameters:
            secret_vars:
              - zitadel_s3_endpoint
              - global_cluster_issuer
  template:
    metadata:
      name: zitadel-seaweedfs-app
      annotations:
        argocd.argoproj.io/sync-wave: "2"
    spec:
      project: zitadel
      destination:
        server: https://kubernetes.default.svc
        namespace: zitadel
      syncPolicy:
        syncOptions:
          - ApplyOutOfSyncOnly=true
        automated:
          prune: true
          selfHeal: true 
      source:
        repoURL: 'https://seaweedfs.github.io/seaweedfs/helm'
        targetRevision: 3.63.0
        chart: seaweedfs
        helm:
          releaseName: zitadel-seaweedfs
          values: |
            global:
              createClusterRole: true
              imageName: chrislusf/seaweedfs
              imagePullPolicy: IfNotPresent
              enableSecurity: false
              securityConfig:
                jwtSigning:
                  volumeWrite: true
                  volumeRead: false
                  filerWrite: false
                  filerRead: false
              serviceAccountName: "zitadel-seaweedfs"
              certificates:
                alphacrds: false
              monitoring:
                enabled: false
                gatewayHost: null
                gatewayPort: null
              enableReplication: false
              replicationPlacment: "001"
              extraEnvironmentVars:
                WEED_CLUSTER_DEFAULT: "sw"
                WEED_CLUSTER_SW_MASTER: "seaweedfs-master.seaweedfs:9333"
                WEED_CLUSTER_SW_FILER: "seaweedfs-filer-client.seaweedfs:8888"
            image:
              registry: ""
              repository: ""
            master:
              enabled: true
              replicas: 1
              port: 9333
              grpcPort: 19333
              metricsPort: 9327
              ipBind: "0.0.0.0"
              loggingOverrideLevel: null
              pulseSeconds: null
              garbageThreshold: null
              metricsIntervalSec: 15
              defaultReplication: "000"
              disableHttp: false
              config: |-
                # Enter any extra configuration for master.toml here.
                # It may be be a multi-line string.
              data:
                type: "existingClaim"
                claimName: "swfs-master-data"
              logs:
                type: "hostPath"
                size: ""
                storageClass: ""
                hostPathPrefix: /storage
            volume:
              enabled: true
              port: 8080
              grpcPort: 18080
              metricsPort: 9327
              ipBind: "0.0.0.0"
              replicas: 1
              loggingOverrideLevel: null
              fileSizeLimitMB: null
              minFreeSpacePercent: 7
              dataDirs:
                - name: data
                  type: "existingClaim"
                  claimName: "swfs-volume-data"
                  maxVolumes: 0
            filer:
              enabled: true
              replicas: 1
              port: 8888
              grpcPort: 18888
              metricsPort: 9327
              encryptVolumeData: false
              data:
                type: "existingClaim"
                claimName: "swfs-filer-data"
              s3:
                enabled: true
                port: 8333
                httpsPort: 0
                allowEmptyFolder: false
                domainName: "{{ .seaweedfs_s3_endpoint }}"
                enableAuth: true
                existingConfigSecret: seaweedfs-s3-secret
            s3:
              enabled: false

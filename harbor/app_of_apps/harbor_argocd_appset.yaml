apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: harbor-app-set
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "4"
spec:
  goTemplate: true
  generators:
    - plugin:
        configMapRef:
          name: secret-var-plugin-generator
        input:
          parameters:
            secret_vars:
              - global_cluster_issuer
              - global_time_zone
              - global_storage_class
              - harbor_hostname
              - harbor_s3_endpoint
              - vouch_hostname

  template:
    # meta data of the Argo CD Application generated by the above ApplicationSet
    metadata:
      name: harbor-web-app
      annotations:
        argocd.argoproj.io/sync-wave: "4"
        argocd.argoproj.io/sync-options: ApplyOnly=true

    spec:
      project: harbor

      # how to reconcile differences should this application get out of date
      syncPolicy:
        syncOptions:
          - ApplyOutOfSyncOnly=true
        automated:
          prune: true
          selfHeal: true

      # where to deploy this to
      destination:
        server: https://kubernetes.default.svc
        namespace: harbor

      # where to get the source of this application
      source:
        repoURL: 'registry-1.docker.io'
        chart: bitnamicharts/valkey
        targetRevision: 24.3.3
        helm:
          releaseName: harbor
          valuesObject:
            fullnameOverride: "harbor"

            ## Format: protocol://domain[:port]. Usually:
            # 1) if "exposureType" is "ingress", the "domain" should be
            # the value of "ingress.hostname"
            #
            # 2) if "exposureType" is "proxy" and "service.type" is "ClusterIP",
            # the "domain" should be the value of "service.clusterIP"
            #
            # 3) if "exposureType" is "proxy" and "service.type" is "NodePort",
            # the "domain" should be the IP address of k8s node
            #
            # 4) if "exposureType" is "proxy" and "service.type" is "LoadBalancer",
            # the "domain" should be the LoadBalancer IP
            externalURL: "https://{{ .harbor_hostname }}"

            # log level used for Harbor services. Allowed values: fatal, error, warn, info, debug, trace
            logLevel: debug

            ## TLS settings
            ## Note: TLS cert files need to provided in each components in advance.
            ##
            internalTLS:
              ## @param internalTLS.enabled Use TLS in all the supported containers: core, jobservice, portal, registry and trivy
              enabled: false
              ## @param internalTLS.caBundleSecret Name of an existing secret with a custom CA that will be injected into the trust store for core, jobservice, registry, trivy components
              ## The secret must contain the key "ca.crt"
              caBundleSecret: ""

            ## @param exposureType The way to expose Harbor. Allowed values are [ ingress \| proxy ]
            ## Use "proxy" to use a deploy NGINX proxy in front of Harbor services
            ## Use "ingress" to use an Ingress Controller as proxy
            ##
            exposureType: ingress

            ingress:
              core:
                hostname: "{{ .harbor_hostname }}"
                ingressClassName: "nginx"
                pathType: ImplementationSpecific
                ## The ingress controller type. Currently supports `default`, `gce` and `ncp`
                ## leave as `default` for most ingress controllers.
                ## set to `gce` if using the GCE ingress controller
                ## set to `ncp` if using the NCP (NSX-T Container Plugin) ingress controller
                controller: default
                annotations:
                  # from the bitnami chart
                  ingress.kubernetes.io/ssl-redirect: "true"
                  ingress.kubernetes.io/proxy-body-size: "0"
                  nginx.ingress.kubernetes.io/ssl-redirect: "true"
                  nginx.ingress.kubernetes.io/proxy-body-size: "0"
                  # from us
                  cert-manager.io/cluster-issuer: "{{ .global_cluster_issuer }}"
                  nginx.ingress.kubernetes.io/auth-signin: "https://{{ .vouch_hostname }}/login?url=$scheme://$http_host$request_uri&vouch-failcount=$auth_resp_failcount&X-Vouch-Token=$auth_resp_jwt&error=$auth_resp_err"
                  nginx.ingress.kubernetes.io/auth-url: "https://{{ .vouch_hostname }}/validate"
                  nginx.ingress.kubernetes.io/auth-response-headers: X-Vouch-User
                  nginx.ingress.kubernetes.io/auth-snippet: |
                    auth_request_set $auth_resp_jwt $upstream_http_x_vouch_jwt;
                    auth_request_set $auth_resp_err $upstream_http_x_vouch_err;
                    auth_request_set $auth_resp_failcount $upstream_http_x_vouch_failcount;

                ## Enable TLS configuration for the host defined at `ingress.core.hostname` parameter
                ## TLS certificates will be retrieved from a TLS secret with name:
                ## `{{- printf "%s-tls" .Values.ingress.core.hostname }}`
                ## You can:
                ##   - Use the `ingress.core.secrets` parameter to create this TLS secret
                ##   - Rely on cert-manager to create it by setting the corresponding annotations
                ##   - Rely on Helm to create self-signed certificates by setting `ingress.core.selfSigned=true`
                tls: true
                ## Create a TLS secret for this ingress record using self-signed certificates
                ## generated by Helm
                selfSigned: false
                ## An array with additional hostname(s) to be covered with the ingress record
                ## e.g:
                ## extraHosts:
                ##   - name: core.harbor.domain
                ##     path: /
                ##
                extraHosts: []
                ## array with additional arbitrary paths that may need to be added to the ingress under the main host
                ## e.g:
                ## extraPaths:
                ## - path: /*
                ##   backend:
                ##     serviceName: ssl-redirect
                ##     servicePort: use-annotation
                ##
                extraPaths: []
                ## TLS configuration for additional hostname(s) to be covered with this ingress record
                ## ref: https://kubernetes.io/docs/concepts/services-networking/ingress/#tls
                ## e.g:
                ## extraTls:
                ## - hosts:
                ##     - core.harbor.domain
                ##   secretName: core.harbor.domain-tls
                ##
                extraTls: []
                ## Custom TLS certificates as secrets
                ## NOTE: 'key' and 'certificate' are expected in PEM format
                ## NOTE: 'name' should line up with a 'secretName' set further up
                ## If it is not set and you're using cert-manager, this is unneeded, as it will create a secret for you with valid certificates
                ## If it is not set and you're NOT using cert-manager either, self-signed certificates will be created valid for 365 days
                ## It is also possible to create and manage the certificates outside of this helm chart
                ## Please see README.md for more information
                ## e.g:
                ## secrets:
                ##   - name: core.harbor.domain-tls
                ##     key: |-
                ##       -----BEGIN RSA PRIVATE KEY-----
                ##       ...
                ##       -----END RSA PRIVATE KEY-----
                ##     certificate: |-
                ##       -----BEGIN CERTIFICATE-----
                ##       ...
                ##       -----END CERTIFICATE-----
                ##
                secrets: []



            # The persistence is enabled by default and a default StorageClass
            # is needed in the k8s cluster to provision volumes dynamically.
            # Specify another StorageClass in the "storageClass" or set "existingClaim"
            # if you already have existing persistent volumes to use
            #
            # For storing images and charts, you can also use "azure", "gcs", "s3",
            # "swift" or "oss". Set it in the "imageChartStorage" section
            persistence:
              enabled: true
              resourcePolicy: "keep"
              persistentVolumeClaim:
                registry:
                  existingClaim: "registry"

                jobservice:
                  jobLog:
                    existingClaim: "jobs"

                trivy:
                  existingClaim: "trivy"
                  storageClass: "{{ .global_storage_class }}"

              # Define which storage backend is used for registry to store
              # images and charts. Refer to
              # https://github.com/distribution/distribution/blob/main/docs/configuration.md#storage
              # for the detail.
              imageChartStorage:
                # Specify whether to disable `redirect` for images and chart storage, for
                # backends which not supported it (such as using minio for `s3` storage type), please disable
                # it. To disable redirects, simply set `disableredirect` to `true` instead.
                # Refer to
                # https://github.com/distribution/distribution/blob/main/docs/configuration.md#redirect
                # for the detail.
                disableredirect: true
                # Specify the "caBundleSecretName" if the storage service uses a self-signed certificate.
                # The secret must contain keys named "ca.crt" which will be injected into the trust store
                # of registry's containers.
                # caBundleSecretName:

                # Specify the type of storage: "filesystem", "azure", "gcs", "s3", "swift",
                # "oss" and fill the information needed in the corresponding section. The type
                # must be "filesystem" if you want to use persistent volumes for registry
                type: s3

                s3:
                  # Set an existing secret for S3 accesskey and secretkey
                  # keys in the secret should be REGISTRY_STORAGE_S3_ACCESSKEY and REGISTRY_STORAGE_S3_SECRETKEY for registry
                  existingSecret: "admin-s3-credentials"
                  region: us-west-1
                  bucket: harbor
                  #regionendpoint: http://myobjects.local
                  #encrypt: false
                  #keyid: mykeyid
                  #secure: true
                  #skipverify: false
                  #v4auth: true
                  #chunksize: "5242880"
                  #rootdirectory: /s3/object/name/prefix
                  #storageclass: STANDARD
                  #multipartcopychunksize: "33554432"
                  #multipartcopymaxconcurrency: 100
                  #multipartcopythresholdsize: "33554432"

            # The initial password of Harbor admin. Change it from portal after launching Harbor
            # or give an existing secret for it
            # key in secret is given via (default to HARBOR_ADMIN_PASSWORD)
            existingSecretAdminPassword: "harbor-admin-credentials"
            existingSecretAdminPasswordKey: password

            # The name of the secret which contains key named "ca.crt". Setting this enables the
            # download link on portal to download the CA certificate when the certificate isn't
            # generated automatically
            caSecretName: ""
            # If using existingSecretSecretKey, the key must be secretKey
            existingSecretSecretKey: "harbor-admin-credentials"

            # Run the migration job via helm hook
            enableMigrateHelmHook: false
            # The custom ca bundle secret, the secret must contain key named "ca.crt"
            # which will be injected into the trust store for core, jobservice, registry, trivy components
            # caBundleSecretName: ""

            ## UAA Authentication Options
            # If you're using UAA for authentication behind a self-signed
            # certificate you will need to provide the CA Cert.
            # Set uaaSecretName below to provide a pre-created secret that
            # contains a base64 encoded CA Certificate named `ca.crt`.
            # uaaSecretName:

            portal:
              image:
                repository: goharbor/harbor-portal
                tag: dev
              # set the service account to be used, default if left empty
              serviceAccountName: ""
              # mount the service account token
              automountServiceAccountToken: false
              replicas: 1
              revisionHistoryLimit: 10
              # resources:
              #  requests:
              #    memory: 256Mi
              #    cpu: 100m
              extraEnvVars: []
              nodeSelector: {}
              tolerations: []
              affinity: {}
              # Spread Pods across failure-domains like regions, availability zones or nodes
              topologySpreadConstraints: []
              # - maxSkew: 1
              #   topologyKey: topology.kubernetes.io/zone
              #   nodeTaintsPolicy: Honor
              #   whenUnsatisfiable: DoNotSchedule
              ## Additional deployment annotations
              podAnnotations: {}
              ## Additional deployment labels
              podLabels: {}
              ## The priority class to run the pod as
              priorityClassName:

            core:
              ## Existing secret for core
              ## The secret must contain the keys:
              ## `secret` (required),
              ## `secretKey` (required),
              existingSecret: "harbor-admin-credentials"
              ## Existing secret for core envvars
              ## The secret must contain the keys:
              ## `CSRF_KEY` (optional - alternatively auto-generated),
              ## `HARBOR_ADMIN_PASSWORD` (optional - alternatively auto-generated),
              ## `POSTGRESQL_PASSWORD` (optional - alternatively uses weak upstream default. Read below if you set it. You must also set postgresql.auth.existingSecret to the same value as core.existingEnvVarsSecret for this to work!),
              ## `postgres-password` (required if POSTGRESQL_PASSWORD is set & must be the same as POSTGRESQL_PASSWORD.)
              ## `HARBOR_DATABASE_PASSWORD` (required if POSTGRESQL_PASSWORD is set & must be the same as POSTGRESQL_PASSWORD.)
              ## `REGISTRY_CREDENTIAL_USERNAME` (optional - alternatively weak defaults),
              ## `REGISTRY_CREDENTIAL_PASSWORD` (optional - alternatively weak defaults),
              ## `_REDIS_URL_CORE` (required - if using the internal Redis - set to base64 of "redis://harbor-redis-master:6379/0")
              ## `_REDIS_URL_REG` (required - if using the internal Redis - set to base64 of "redis://harbor-redis-master:6379/2")
              ##
              ## If you do not know how to start, let the chart generate a full secret for you before defining an existingEnvVarsSecret
              ## Notes:
              ##   As a EnvVars secret, this secret also store redis config urls
              ##   The HARBOR_ADMIN_PASSWORD is only required at initial deployment, once the password is set in database, it is not used anymore
              existingEnvVarsSecret: "harbor-registry-credentials"

            jobservice:
              # Secret is used when job service communicates with other components.
              # If a secret key is not specified, Helm will generate one.
              # Must be a string of 16 chars.
              secret: ""
              ## @param jobservice.existingSecret Existing secret for jobservice
              ## The secret must contain the keys:
              ## `secret` (required),
              ##
              existingSecret: "harbor-admin-credentials"

            registry:
              credentials:
                username: "harbor_registry_user"
                password: "harbor_registry_password"
                # Login and password in htpasswd string format. Excludes `registry.credentials.username`  and `registry.credentials.password`. May come in handy when integrating with tools like argocd or flux. This allows the same line to be generated each time the template is rendered, instead of the `htpasswd` function from helm, which generates different lines each time because of the salt.
                # htpasswd: $apr1$XLefHzeG$Xl4.s00sMSCCcMyJljSZb0 # example string
                # If using existingSecret, the key must be REGISTRY_PASSWD and REGISTRY_HTPASSWD
                existingSecret: ""

            trivy:
              # enabled the flag to enable Trivy scanner
              enabled: true
              ## @param trivy.existingEnvVarsSecret Existing secret for trivy
              ## The secret must contain the keys:
              ## `SCANNER_TRIVY_GITHUB_TOKEN` (optional)
              ## `SCANNER_REDIS_URL` (required - if using the internal Redis - set to base64 of "redis://harbor-redis-master:6379/5")
              ## `SCANNER_STORE_REDIS_URL` (required - if using the internal Redis - set to base64 of "redis://harbor-redis-master:6379/5")
              ## `SCANNER_JOB_QUEUE_REDIS_URL` (required - if using the internal Redis - set to base64 of "redis://harbor-redis-master:6379/5")
              ##
              existingEnvVarsSecret: ""

            externalDatabase:
              host: "harbor-postgrees-rw"
              port: "5432"
              user: "harbor"
              coreDatabase: "registry"
              # if using existing secret, the key must be "password"
              existingSecret: "harbor-pgsql-credentials"
              existingSecretPasswordKey: "password"
              # "disable" - No SSL
              # "require" - Always SSL (skip verification)
              # "verify-ca" - Always SSL (verify that the certificate presented by the
              # server was signed by a trusted CA)
              # "verify-full" - Always SSL (verify that the certification presented by the
              # server was signed by a trusted CA and the server host name matches the one
              # in the certificate)
              sslmode: "disable"

            externalRedis:
              host: "valkey-primary"
              port: 6379
              password: ""
              # If using existingSecret, the key must be REDIS_PASSWORD
              existingSecret: "harbor-valkey-credentials"
              coreDatabaseIndex: "0"
              jobserviceDatabaseIndex: "1"
              registryDatabaseIndex: "2"
              trivyAdapterDatabaseIndex: "5"
              ## Redis&reg; sentinel configuration
              sentinel:
                ## If external redis with sentinal is used, set it to `true`
                enabled: false
                ## Name of sentinel masterSet if sentinel is used
                masterSet: "mymaster"
                ## Sentinel hosts and ports in the format
                hosts: ""

            exporter:
              replicaCount: 1
              revisionHistoryLimit: 2
              # resources:
              #  requests:
              #    memory: 256Mi
              #    cpu: 100m

            metrics:
              enabled: true
              core:
                path: /metrics
                port: 8001
              registry:
                path: /metrics
                port: 8001
              jobservice:
                path: /metrics
                port: 8001
              exporter:
                path: /metrics
                port: 8001
              ## Create prometheus serviceMonitor to scrape harbor metrics.
              ## This requires the monitoring.coreos.com/v1 CRD. Please see
              ## https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/getting-started.md
              ##
              serviceMonitor:
                enabled: true

            # cache layer configurations
            # if this feature enabled, harbor will cache the resource
            # `project/project_metadata/repository/artifact/manifest` in the redis
            # which help to improve the performance of high concurrent pulling manifest.
            cache:
              # default is not enabled.
              enabled: false
              # default keep cache for one day.
              expireHours: 24

apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: alertmanager-appset
  namespace: argocd
spec:
  goTemplate: true
  # generator allows us to source specific values from an external secret
  generators:
    - plugin:
        configMapRef:
          name: secret-var-plugin-generator
        input:
          parameters:
            secret_vars:
              - prometheus_hostname
              - prometheus_alert_manager_hostname
              - prometheus_grafana_hostname
              - zitadel_hostname
              - vouch_hostname
              - global_cluster_issuer
  template:
    metadata:
      name: alertmanager-app
    spec:
      project: default
      syncPolicy:
        syncOptions:
          - CreateNamespace=true
          - ApplyOutOfSyncOnly=true
          - Retry=true
        automated:
          selfHeal: false
          prune: true
      destination:
        server: "https://kubernetes.default.svc"
        namespace: monitoring
      source:
        repoURL: 'https://prometheus-community.github.io/helm-charts'
        chart: alertmanager
        targetRevision: 1.21.0
        helm:
          version: v3
          skipCrds: true
          valuesObject:
            replicaCount: 1

            image:
              repository: quay.io/prometheus/alertmanager
              pullPolicy: IfNotPresent
              tag: ""

            baseURL: 'https://{{ .prometheus_alert_manager_hostname }}'

            extraArgs: {}

            extraSecretMounts: []
              # - name: secret-files
              #   mountPath: /etc/secrets
              #   subPath: ""
              #   secretName: alertmanager-secret-files
              #   readOnly: true

            nameOverride: ""
            fullnameOverride: ""

            automountServiceAccountToken: true

            serviceAccount:
              create: true
              annotations: {}
              # The name of the service account to use.
              # If not set and create is true, a name is generated using the fullname template
              name: ""

            # Sets schedulerName in alertmanager pod
            schedulerName: ""

            podSecurityContext:
              fsGroup: 65534

            dnsConfig: {}
              # nameservers:
              #   - 1.2.3.4
              # searches:
              #   - ns1.svc.cluster-domain.example
              #   - my.dns.search.suffix
              # options:
              #   - name: ndots
              #     value: "2"
              #   - name: edns0
            hostAliases: []
              # - ip: "127.0.0.1"
              #   hostnames:
              #   - "foo.local"
              #   - "bar.local"
              # - ip: "10.1.2.3"
              #   hostnames:
              #   - "foo.remote"
              #   - "bar.remote"
            securityContext:
              # capabilities:
              #   drop:
              #   - ALL
              # readOnlyRootFilesystem: true
              runAsUser: 65534
              runAsNonRoot: true
              runAsGroup: 65534

            additionalPeers: []

            service:
              annotations: {}
              labels: {}
              type: ClusterIP
              port: 9093
              clusterPort: 9094
              loadBalancerIP: ""  # Assign ext IP when Service type is LoadBalancer
              loadBalancerSourceRanges: []  # Only allow access to loadBalancerIP from these IPs
              # if you want to force a specific nodePort. Must be use with service.type=NodePort
              # nodePort:

              # Optionally specify extra list of additional ports exposed on both services
              extraPorts: []

              # ip dual stack
              ipDualStack:
                enabled: false
                ipFamilies: ["IPv6", "IPv4"]
                ipFamilyPolicy: "PreferDualStack"

            ingress:
              enabled: true
              className: "nginx"
              labels: {}
              annotations:
                cert-manager.io/cluster-issuer: '{{ .global_cluster_issuer }}'
                nginx.ingress.kubernetes.io/auth-signin: 'https://{{ .vouch_hostname }}/login?url=$scheme://$http_host$request_uri&vouch-failcount=$auth_resp_failcount&X-Vouch-Token=$auth_resp_jwt&error=$auth_resp_err'
                nginx.ingress.kubernetes.io/auth-url: 'https://{{ .vouch_hostname }}/validate'
                nginx.ingress.kubernetes.io/auth-response-headers: X-Vouch-User
                nginx.ingress.kubernetes.io/auth-snippet: |
                  auth_request_set $auth_resp_jwt $upstream_http_x_vouch_jwt;
                  auth_request_set $auth_resp_err $upstream_http_x_vouch_err;
                  auth_request_set $auth_resp_failcount $upstream_http_x_vouch_failcount;
              hosts:
                - host: '{{ .prometheus_alert_manager_hostname }}'
                  paths:
                    - path: /
                      pathType: ImplementationSpecific
              tls:
                - secretName: alertmanager-tls
                  hosts:
                  - '{{ .prometheus_alert_manager_hostname }}'


            resources: {}
              # limits:
              #   cpu: 100m
              #   memory: 128Mi
              # requests:
              #   cpu: 10m
              #   memory: 32Mi

            nodeSelector: {}

            tolerations: []

            affinity: {}

            podAntiAffinity: ""

            podAntiAffinityTopologyKey: kubernetes.io/hostname

            topologySpreadConstraints: []

            statefulSet:
              annotations: {}

            minReadySeconds: 0

            podAnnotations: {}
            podLabels: {}

            podDisruptionBudget: {}

            command: []

            persistence:
              enabled: false
              storageClass: "local-path"
              accessModes:
                - ReadWriteOnce
              size: 50Mi

            configAnnotations: {}

            config:
              enabled: true
              global:
                resolve_timeout: 5m

                inhibit_rules:
                  - source_matchers:
                      - 'severity = critical'
                    target_matchers:
                      - 'severity =~ info'
                    equal:
                      - 'namespace'
                      - 'alertname'

                  - source_matchers:
                      - 'severity = warning'
                    target_matchers:
                      - 'severity =~ info'
                    equal:
                      - 'namespace'
                      - 'alertname'

                  - source_matchers:
                      - 'alertname = InfoInhibitor'
                    target_matchers:
                      - 'severity =~ info'
                    equal:
                      - 'namespace'

                  - target_matchers:
                      - 'alertname = InfoInhibitor'

                  - target_matchers:
                      - 'alertname = Watchdog'

                  - target_matchers:
                      - 'alertname = KubeVersionMismatch'

              templates:
                - '/etc/alertmanager/*.tmpl'

              receivers:
                - name: default-receiver
                  # slack_configs:
                  #  - channel: '@you'
                  #    send_resolved: true

              route:
                group_wait: 10s
                group_interval: 5m
                receiver: default-receiver
                repeat_interval: 3h

            ## Monitors ConfigMap changes and POSTs to a URL
            ## Ref: https://github.com/prometheus-operator/prometheus-operator/tree/main/cmd/prometheus-config-reloader
            ##
            configmapReload:
              ## If false, the configmap-reload container will not be deployed
              ##
              enabled: true

              ## configmap-reload container name
              ##
              name: configmap-reload

              ## configmap-reload container image
              ##
              image:
                repository: quay.io/prometheus-operator/prometheus-config-reloader
                tag: v0.83.0
                pullPolicy: IfNotPresent

              # containerPort: 9533

              ## configmap-reload resource requests and limits
              ## Ref: http://kubernetes.io/docs/user-guide/compute-resources/
              ##
              resources: {}

              livenessProbe: {}
                # httpGet:
                #   path: /healthz
                #   port: 8080
                #   scheme: HTTP
              readinessProbe: {}
                # httpGet:
                #   path: /healthz
                #   port: 8080
                #   scheme: HTTP

              extraArgs: {}

              ## Optionally specify extra list of additional volumeMounts
              extraVolumeMounts: []
                # - name: extras
                #   mountPath: /usr/share/extras
                #   readOnly: true

              ## Optionally specify extra environment variables to add to alertmanager container
              extraEnv: []
                # - name: FOO
                #   value: BAR

              securityContext: {}
                # capabilities:
                #   drop:
                #   - ALL
                # readOnlyRootFilesystem: true
                # runAsUser: 65534
                # runAsNonRoot: true
                # runAsGroup: 65534

            templates: {}
            #   alertmanager.tmpl: |-

            ## Optionally specify extra list of additional volumeMounts
            extraVolumeMounts: []
              # - name: extras
              #   mountPath: /usr/share/extras
              #   readOnly: true

            ## Optionally specify extra list of additional volumes
            extraVolumes: []
              # - name: extras
              #   emptyDir: {}

            ## Optionally specify extra environment variables to add to alertmanager container
            extraEnv: []
              # - name: FOO
              #   value: BAR

            testFramework:
              enabled: false
              annotations:
                "helm.sh/hook": test-success
                # "helm.sh/hook-delete-policy": "before-hook-creation,hook-succeeded"

            # --- Vertical Pod Autoscaler
            verticalPodAutoscaler:
              # -- Use VPA for alertmanager
              enabled: false
              # recommenders:
              #   - name: 'alternative'
              # updatePolicy:
              #   updateMode: "Auto"
              #   minReplicas: 1
              # resourcePolicy:
              #   containerPolicies:
              #     - containerName: '*'
              #       minAllowed:
              #         cpu: 100m
              #         memory: 128Mi
              #       maxAllowed:
              #         cpu: 1
              #         memory: 500Mi
              #       controlledResources: ["cpu", "memory"]

            # --- Extra Pod Configs
            extraPodConfigs: {}
              # dnsPolicy: ClusterFirstWithHostNet
              # hostNetwork: true

# installs whole kube-prometheus-stack which includes grafana and alert manager
---
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: prometheus-appset
  namespace: argocd
  annotations:
    argocd.argoproj.io/sync-wave: "2"
spec:
  goTemplate: true
  # generator allows us to source specific values from an external secret
  generators:
    - plugin:
        configMapRef:
          name: secret-var-plugin-generator
        input:
          parameters:
            secret_vars:
              - prometheus_hostname
              - prometheus_alert_manager_hostname
              - prometheus_grafana_hostname
              - zitadel_hostname
              - vouch_hostname
              - global_cluster_issuer
  template:
    metadata:
      name: prometheus-stack-helm-chart
    spec:
      project: default
      syncPolicy:
        syncOptions:
          - ApplyOutOfSyncOnly=true
        automated:
          selfHeal: false
          prune: false
      destination:
        server: https://kubernetes.default.svc
        namespace: monitoring
      source:
        repoURL: https://prometheus-community.github.io/helm-charts
        chart: kube-prometheus-stack
        targetRevision: 67.1.0
        helm:
          version: v3
          releaseName: prom-stack
          valuesObject:
            fullnameOverride: prom-stack
            global:
              evaluation_interval: 5s
              scrape_interval: 5s
              scrape_timeout: 10s

            crds:
              enabled: false

            ## Provide custom recording or alerting rules to be deployed into the cluster.
            additionalPrometheusRulesMap:
              # basic Argo CD alerts from awesome prometheus alerts
              prometheus.rules:
                groups:
                  - name: argo_cd
                    rules:
                      - alert: ArgocdServiceNotSynced
                        expr: argocd_app_info{sync_status!="Synced"} != 0
                        for: 15m
                        labels:
                          severity: warning
                        annotations:
                          summary: ArgoCD service not synced (instance {{`{{ $labels.instance }}`}})
                          description: "Service {{`{{ $labels.name }}`}} run by argo is currently not in sync.\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"

                      - alert: ArgocdServiceUnhealthy
                        expr: argocd_app_info{health_status!="Healthy"} != 0
                        for: 15m
                        labels:
                          severity: warning
                        annotations:
                          summary: ArgoCD service unhealthy (instance {{`{{ $labels.instance }}`}})
                          description: "Service {{`{{ $labels.name }}`}} run by argo is currently not healthy.\n  VALUE = {{`{{ $value }}`}}\n  LABELS = {{`{{ $labels }}`}}"

            prometheus:
              ingress:
                enabled: true
                ingressClassName: nginx
                annotations:
                  cert-manager.io/cluster-issuer: '{{ .global_cluster_issuer }}'
                  nginx.ingress.kubernetes.io/auth-signin: 'https://{{ .vouch_hostname }}/login?url=$scheme://$http_host$request_uri&vouch-failcount=$auth_resp_failcount&X-Vouch-Token=$auth_resp_jwt&error=$auth_resp_err'
                  nginx.ingress.kubernetes.io/auth-url: 'https://{{ .vouch_hostname }}/validate'
                  nginx.ingress.kubernetes.io/auth-response-headers: X-Vouch-User
                  nginx.ingress.kubernetes.io/auth-snippet: |
                    auth_request_set $auth_resp_jwt $upstream_http_x_vouch_jwt;
                    auth_request_set $auth_resp_err $upstream_http_x_vouch_err;
                    auth_request_set $auth_resp_failcount $upstream_http_x_vouch_failcount;
                    # when VP is hosted externally to k8s ensure the SSL cert is valid to avoid MITM risk
                    proxy_ssl_trusted_certificate /etc/ssl/certs/ca-certificates.crt;
                    proxy_ssl_session_reuse on;
                    proxy_ssl_verify_depth 2;
                    proxy_ssl_verify on;
                hosts:
                  - '{{ .prometheus_hostname }}'
                paths:
                  - /
                tls:
                  - secretName: prometheus-tls
                    hosts:
                    - '{{ .prometheus_hostname }}'

              thanosService:
                # For use with SideCar only
                enabled: false
                annotations: {}
                labels: {}

              thanosServiceMonitor:
                # For use with SideCar only
                enabled: false

              disableCompaction: true

              prometheusSpec:
                # For use with Thanos SideCar only
                # thanos:
                #   create: false
                #   image: quay.io/thanos/thanos:v0.28.1
                #   objectStorageConfig:
                #     existingSecret:
                #       name: thanos-objstore-config
                #       key: thanos.yaml

                enabledRemoteWriteReceiver: false
                remoteWriteDashboards: false

                remoteWrite:
                  - url: http://thanos-receive:19291/api/v1/receive

                probeSelectorNilUsesHelmValues: false
                probeSelector: {}
                probeNamespaceSelector: {}

                podMonitorSelectorNilUsesHelmValues: false
                podMonitorSelector: {}
                podMonitorNamespaceSelector: {}

                ruleSelectorNilUsesHelmValues: false
                ruleSelector: {}
                ruleNamespaceSelector: {}

                serviceMonitorSelectorNilUsesHelmValues: false
                serviceMonitorSelector: {}
                serviceMonitorNamespaceSelector: {}

                additionalScrapeConfigsSecret:
                  enabled: false
                  name: additional-scrape-configs
                  key: scrape-targets.yaml

                storageSpec:
                  volumeClaimTemplate:
                    spec:
                      storageClassName: local-path
                      accessModes: ["ReadWriteOnce"]
                      resources:
                        requests:
                          storage: 32Gi

                retention: 7d
                logLevel: info

              prometheusOperator:
                namespaces:
                  releaseNamespace: true
                  additional:
                    - kube-system
                    - nextcloud
                    - k8up
                    - zitadel
                admissionWebhooks:
                  certManager:
                    enabled: false
                  patch:
                    enabled: false

            grafana:
              plugins:
                - redis-datasource
                - grafana-piechart-panel
                - grafana-lokiexplore-app

              datasources:
                enabled: true
                defaultDatasourceEnabled: true
                isDefaultDatasource: true
                name: Thanos
                uid: thanos
                url: http://thanos-query-frontend.monitoring.svc.cluster.local:9090

              additionalDataSources:
                - name: loki
                  access: proxy
                  editable: false
                  type: loki
                  url: http://loki-query-frontend.monitoring.svc.cluster.local:3100
                  jsonData:
                    timeout: 60
                    maxLines: 1000

              envValueFrom:
                GF_AUTH_GENERIC_OAUTH_CLIENT_ID:
                  secretKeyRef:
                    name: grafana-oidc-credentials
                    key: client_id

                GF_AUTH_GENERIC_OAUTH_CLIENT_SECRET:
                  secretKeyRef:
                    name: grafana-oidc-credentials
                    key: client_secret

              grafana.ini:
                auth:
                  disable_login_form: false
                  publicDashboards: true

                users:
                  allow_sign_up: false
                  auto_assign_org: true
                  auto_assign_org_role: Admin

                "auth.generic_oauth":
                  name: Zitadel
                  enabled: true
                  scopes: openid email profile groups
                  attribute_path: email
                  login_attribute: preferred_username
                  name_attribute: name
                  auth_url: https://{{ .zitadel_hostname }}/oauth/v2/authorize
                  token_url: https://{{ .zitadel_hostname }}/oauth/v2/token
                  api_url: https://{{ .zitadel_hostname }}/oauth/v1/userinfo
                users:
                  allow_sign_up: false
                  auto_assign_org: true
                  auto_assign_org_role: Admin
                server:
                  root_url: "https://{{ .prometheus_grafana_hostname }}"

              ingress:
                enabled: true
                ingressClassName: nginx
                annotations:
                  cert-manager.io/cluster-issuer: '{{ .global_cluster_issuer }}'
                hosts:
                  - '{{ .prometheus_grafana_hostname }}'
                tls:
                  - secretName: grafana-tls
                    hosts:
                    - '{{ .prometheus_grafana_hostname }}'

            scrape_configs:
            - job_name: ingress
              honor_timestamps: true
              scrape_interval: 1m
              scrape_timeout: 1m
              metrics_path: /metrics
              scheme: http

            defaultRules:
              create: true
              rules:

                kubeControllerManager:
                  enabled: false
                  service:
                    enabled: false
                  serviceMonitor:
                    enabled: false

                kubeScheduler:
                  enabled: false
                  service:
                    enabled: false
                  serviceMonitor:
                    enabled: false

            alertmanager:
              enabled: true
              annotations: {}
              ingress:
                enabled: true
                ingressClassName: nginx
                annotations:
                  cert-manager.io/cluster-issuer: '{{ .global_cluster_issuer }}'
                  nginx.ingress.kubernetes.io/auth-signin: 'https://{{ .vouch_hostname }}/login?url=$scheme://$http_host$request_uri&vouch-failcount=$auth_resp_failcount&X-Vouch-Token=$auth_resp_jwt&error=$auth_resp_err'
                  nginx.ingress.kubernetes.io/auth-url: 'https://{{ .vouch_hostname }}/validate'
                  nginx.ingress.kubernetes.io/auth-response-headers: X-Vouch-User
                  nginx.ingress.kubernetes.io/auth-snippet: |
                    auth_request_set $auth_resp_jwt $upstream_http_x_vouch_jwt;
                    auth_request_set $auth_resp_err $upstream_http_x_vouch_err;
                    auth_request_set $auth_resp_failcount $upstream_http_x_vouch_failcount;
                hosts:
                  - '{{ .prometheus_alert_manager_hostname }}'
                paths:
                  - /
                tls:
                  - secretName: alertmanager-tls
                    hosts:
                    - '{{ .prometheus_alert_manager_hostname }}'

              alertmanagerSpec:
                retention: 12h
                storage: {}
                externalUrl: 'https://{{ .prometheus_alert_manager_hostname }}'

              config:
                global:
                  resolve_timeout: 5m

                  inhibit_rules:
                    - source_matchers:
                        - 'severity = critical'
                      target_matchers:
                        - 'severity =~ info'
                      equal:
                        - 'namespace'
                        - 'alertname'

                    - source_matchers:
                        - 'severity = warning'
                      target_matchers:
                        - 'severity =~ info'
                      equal:
                        - 'namespace'
                        - 'alertname'

                    - source_matchers:
                        - 'alertname = InfoInhibitor'
                      target_matchers:
                        - 'severity =~ info'
                      equal:
                        - 'namespace'

                    - target_matchers:
                        - 'alertname = InfoInhibitor'

                    - target_matchers:
                        - 'alertname = Watchdog'

                    - target_matchers:
                        - 'alertname = KubeVersionMismatch'

